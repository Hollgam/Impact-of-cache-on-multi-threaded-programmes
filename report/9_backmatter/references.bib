@misc{Aas2005,
author = {Aas, Josh},
title = {{Understanding the Linux 2.6.8.1 CPU Scheduler}},
url = {http://joshaas.net/linux/linux\_cpu\_scheduler.pdf},
urldate = {12/05/14},
year = {2005}
}
@article{Abrial2007,
abstract = {This paper gives a tutorial introduction to the ideas behind system development using the B-Method. Properly handled, the crucial relationship between requirements and formal model leads to systems that are correct by construction. Some industrial successes are outlined.},
author = {Abrial, Jean-Raymond},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Abrial/Journal of Universal Computer Science/Abrial - 2007 - Formal Methods Theory Becoming Practice.pdf:pdf},
journal = {Journal of Universal Computer Science},
keywords = {b-method,formal methods},
pages = {619--628},
title = {{Formal Methods: Theory Becoming Practice}},
url = {http://jucs.org/jucs\_13\_5/formal\_methods\_theory\_becoming/jucs\_13\_5\_0619\_0628\_abrial.pdf},
volume = {13},
year = {2007}
}
@article{Agarwal1989,
author = {Agarwal, A. and Hennessy, J. and Horowitz, M.},
doi = {10.1145/63404.63407},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Agarwal, Hennessy, Horowitz/ACM Transactions on Computer Systems/Agarwal, Hennessy, Horowitz - 1989 - An analytical cache model.pdf:pdf},
issn = {07342071},
journal = {ACM Transactions on Computer Systems},
month = may,
number = {2},
pages = {184--215},
publisher = {ACM},
title = {{An analytical cache model}},
url = {http://dl.acm.org/citation.cfm?id=63404.63407},
volume = {7},
year = {1989}
}
@article{agarwal1992performance,
author = {Agarwal, Anant},
journal = {Parallel and Distributed Systems, IEEE Transactions on},
number = {5},
pages = {525--539},
publisher = {IEEE},
title = {{Performance tradeoffs in multithreaded processors}},
volume = {3},
year = {1992}
}
@misc{Akers2012,
abstract = {A diversity of user goals and strategies make creation-oriented applications such as word processors or photo-editors difficult to comprehensively test. Evaluating such applications requires testing a large pool of participants to capture the diversity of experience, but traditional usability testing can be prohibitively expensive. To address this problem, this article contributes a new usability evaluation method called backtracking analysis, designed to automate the process of detecting and characterizing usability problems in creation-oriented applications. The key insight is that interaction breakdowns in creation-oriented applications often manifest themselves in backtracking operations that can be automatically logged (e.g., undo and erase operations). Backtracking analysis synchronizes these events to contextual data such as screen capture video, helping the evaluator to characterize specific usability problems. The results from three experiments demonstrate that backtracking events can be effective indicators of usability problems in creation-oriented applications, and can yield a cost-effective alternative to traditional laboratory usability testing.},
author = {Akers, David and Jeffries, Robin and Simpson, Matthew and Winograd, Terry},
booktitle = {ACM Transactions on Computer-Human Interaction},
doi = {10.1145/2240156.2240164},
isbn = {1073-0516},
issn = {10730516},
pages = {1--40},
title = {{Backtracking Events as Indicators of Usability Problems in Creation-Oriented Applications}},
volume = {19},
year = {2012}
}
@misc{AMD2005,
author = {AMD},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/AMD/Unknown/AMD - 2005 - Multi-core processors - the next evolution in computing.pdf:pdf},
title = {{Multi-core processors - the next evolution in computing}},
url = {http://static.highspeedbackbone.net/pdf/AMD\_Athlon\_Multi-Core\_Processor\_Article.pdf},
urldate = {08/02/14},
year = {2005}
}
@article{Archibald1986,
author = {Archibald, James and Baer, Jean-Loup},
doi = {10.1145/6513.6514},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Archibald, Baer/ACM Transactions on Computer Systems/Archibald, Baer - 1986 - Cache coherence protocols evaluation using a multiprocessor simulation model.pdf:pdf},
issn = {07342071},
journal = {ACM Transactions on Computer Systems},
month = sep,
number = {4},
pages = {273--298},
publisher = {ACM},
title = {{Cache coherence protocols: evaluation using a multiprocessor simulation model}},
url = {http://dl.acm.org/citation.cfm?id=6513.6514},
volume = {4},
year = {1986}
}
@misc{Arpaci-Dusseau2014,
author = {Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau, Andrea C.},
title = {{Operating Systems: Three Easy Pieces}},
url = {http://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf},
urldate = {12/05/14},
year = {2014}
}
@misc{BackTrack2014,
author = {BackTrack},
keywords = {Penetration Testing,Pentest,exploits,hacking,information security training},
title = {http://www.backtrack-linux.org/},
url = {http://www.backtrack-linux.org/},
urldate = {07/05/14},
year = {2014}
}
@misc{Balakrishnan2010,
author = {Balakrishnan, Ganesh and Begun, Ralph M. and Kochuparambil, Bejoy},
title = {{Understanding Intel Xeon 5600 Series Memory Performance and Optimization in IBM System x and BladeCenter Platforms}},
url = {http://public.dhe.ibm.com/common/ssi/ecm/en/xsw03075usen/XSW03075USEN.PDF},
urldate = {29/04/14},
year = {2010}
}
@misc{Barney2013,
author = {Barney, Blaise},
keywords = {Blaise Barney,HPC,High Performance Computing,LLNL,Lawrence LivermoreNational Laboratory,Posix threads,parallel,programming,pthreads,training,tutorials,workshops},
title = {{POSIX Threads Programming}},
url = {https://computing.llnl.gov/tutorials/pthreads/},
urldate = {13/05/14},
year = {2013}
}
@misc{Bart2014,
author = {Bart, Jacobs and Smans, Jan and Piessens, Frank},
title = {{VeriFast}},
url = {http://people.cs.kuleuven.be/~bart.jacobs/verifast/},
urldate = {23/05/14},
year = {2014}
}
@phdthesis{Bazilinskyy2013,
author = {Bazilinskyy, Pavlo},
school = {University of St Andrews},
title = {{Multi-core Insense}},
year = {2013}
}
@misc{Blelloch1999,
abstract = {Many high-level parallel programming languages allow for fine-grained parallelism. As in the popular work-time framework for parallel algorithm design, programs written in such languages can express the full parallelism in the program without specifying the mapping of program tasks to processors. A common concern in executing such programs is to schedule tasks to processors dynamically so as to minimize not only the execution time, but also the amount of space (memory) needed. Without careful scheduling, the parallel execution on p processors can use a factor of p or larger more space than a sequential implementation of the same program. This paper first identifies a class of parallel schedules that are provably efficient in both time and space. For any computation with Pub Fmt italic w Pub Fmt /italic units of work and critical path length Pub Fmt italic d Pub Fmt /italic , and for any sequential schedule that takes space s 1 , we provide a parallel schedule that takes fewer than w/p + d steps on p processors and requires less than s 1 + p\&dot;d space. This matches the lower bound that we show, and significantly improves upon the best previous bound of s 1 \&dot;p spaces for the common case where d << s 1 . The paper then describes a scheduler for implementing high-level languages with nested parallelism, that generates schedules in this class. During program execution, as the structure of the computation is revealed, the scheduler keeps track of the active tasks, allocates the tasks to the processors, and performs the necessary task synchronization. The scheduler is itself a parallel algorithm, and incurs at most a constant factor overhead in time and space, even when the scheduling granularity is individual units of work. The algorithm is the first efficient solution to the scheduling problem discussed here, even if space considerations are ignored.},
author = {Blelloch, Guy E. and Gibbons, Phillip B. and Matias, Yossi},
booktitle = {Journal of the ACM},
doi = {10.1145/301970.301974},
isbn = {0897917170},
issn = {00045411},
pages = {281--321},
title = {{Provably efficient scheduling for languages with fine-grained parallelism}},
volume = {46},
year = {1999}
}
@article{Blumofe1994,
abstract = {This paper studies the problem of efficiently scheduling fully
strict (i.e., well-structured) multithreaded computations on parallel
computers. A popular and practical method of scheduling this kind of
dynamic MIMD-style computation is \&amp;ldquo;work stealing,\&amp;rdquo; in which
processors needing work steal computational threads from other
processors. In this paper, we give the first provably good work-stealing
scheduler for multithreaded computations with dependencies.
Specifically, our analysis shows that the expected time T<sub>P</sub> to
execute a fully strict computation on P processors using our
work-stealing scheduler is T<sub>P</sub>=O(T<sub>1</sub>/P+T<sub>\&amp;infin;
</sub>), where T<sub>1</sub> is the minimum serial execution time of the
multithreaded computation and T<sub>\&amp;infin;</sub> is the minimum
execution time with an infinite number of processors. Moreover, the
space S<sub>P</sub> required by the execution satisfies
S<sub>P</sub>\&amp;les;S<sub>1</sub>P. We also show that the expected total
communication of the algorithm is at most O(T<sub>\&amp;infin;</sub>S<sub>max
</sub>P), where S<sub>max</sub> is the size of the largest activation
record of any thread, thereby justifying the folk wisdom that
work-stealing schedulers are more communication efficient than their
work-sharing counterparts. All three of these bounds are existentially
optimal to within a constant factor},
author = {Blumofe, R.D. and Leiserson, C.E.},
doi = {10.1109/SFCS.1994.365680},
isbn = {0-8186-6580-7},
issn = {00045411},
journal = {Proceedings 35th Annual Symposium on Foundations of Computer Science},
title = {{Scheduling multithreaded computations by work stealing}},
year = {1994}
}
@book{Bovet:2005:ULK:1077084,
author = {Bovet, Daniel and Cesati, Marco},
isbn = {0596005652},
publisher = {Oreilly \& Associates Inc},
title = {{Understanding The Linux Kernel}},
year = {2005}
}
@misc{Boyd-Wickizer2009,
author = {Boyd-Wickizer, Silas and Morris, Robert and Kaashoek, M. Frans},
title = {{Reinventing Scheduling for Multicore Systems}},
url = {https://www.usenix.org/legacy/event/hotos09/tech/full\_papers/boyd-wickizer/boyd-wickizer.pdf?origin=publication\_detail},
urldate = {13/05/14},
year = {2009}
}
@book{Butenhof1997,
author = {Butenhof, David R.},
isbn = {0201633922},
pages = {400},
publisher = {Addison-Wesley Professional},
title = {{Programming with POSIX Threads}},
url = {http://www.amazon.com/Programming-POSIX-Threads-David-Butenhof/dp/0201633922},
year = {1997}
}
@inproceedings{Cacaval2003,
address = {New York, New York, USA},
author = {CaΒcaval, Calin and Padua, David A.},
booktitle = {Proceedings of the 17th annual international conference on Supercomputing - ICS '03},
doi = {10.1145/782814.782836},
isbn = {1581137338},
keywords = {cache modeling,compiler algorithms,stack algorithms},
month = jun,
pages = {150},
publisher = {ACM Press},
title = {{Estimating cache misses and locality using stack distances}},
url = {http://dl.acm.org/citation.cfm?id=782814.782836},
year = {2003}
}
@article{ChenDing,
author = {{Chen Ding}, Trishul Chilimbi},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Chen Ding/Unknown/Chen Ding - 2009 - A composable model for analyzing locality of multi-threaded programs.pdf:pdf},
title = {{A composable model for analyzing locality of multi-threaded programs}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.187.7582},
year = {2009}
}
@inproceedings{Chen2007,
address = {New York, New York, USA},
author = {Chen, Shimin and Mowry, Todd C. and Wilkerson, Chris and Gibbons, Phillip B. and Kozuch, Michael and Liaskovitis, Vasileios and Ailamaki, Anastassia and Blelloch, Guy E. and Falsafi, Babak and Fix, Limor and Hardavellas, Nikos},
booktitle = {Proceedings of the nineteenth annual ACM symposium on Parallel algorithms and architectures - SPAA '07},
doi = {10.1145/1248377.1248396},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Chen et al/Proceedings of the nineteenth annual ACM symposium on Parallel algorithms and architectures - SPAA '07/Chen et al. - 2007 - Scheduling threads for constructive cache sharing on CMPs.pdf:pdf},
isbn = {9781595936677},
keywords = {chip multiprocessors,constructive cache sharing,parallel depth first,scheduling algorithms,thread granularity,work stealing,working set profiling},
month = jun,
pages = {105},
publisher = {ACM Press},
title = {{Scheduling threads for constructive cache sharing on CMPs}},
url = {http://dl.acm.org/citation.cfm?id=1248377.1248396},
year = {2007}
}
@article{Conway2011,
author = {Conway, Pat and Kalyanasundharam, Nathan and Donley, Gregg and Lepak, Kevin and Hughes, Bill},
journal = {Intelligent Systems},
number = {March/April},
pages = {17 -- 29},
title = {{Cache Hierarchy and Memory Subsystem of the AMD Opteron Processor}},
url = {http://portal.nersc.gov/project/training/files/XE6-feb-2011/Architecture/Opteron-Memory-Cache.pdf},
volume = {1},
year = {2011}
}
@misc{World2014,
author = {{CPU World}},
keywords = {CPU,CPU comparison,FAQ,Intel,Xeon E5-2695 v2 - CM8063501288706 / BX80635E52695V,information,microprocessor,news,specifications},
title = {{Specifications of Intel Xeon E5-2695 v2}},
url = {http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon E5-2695 v2.html},
urldate = {06/05/14},
year = {2014}
}
@misc{D.Muntz1991,
author = {{D. Muntz} and Honeyman, P.},
booktitle = {CITI Technical Report},
title = {{Multi-level Caching in Distributed File Systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.2975\&rep=rep1\&type=pdf},
urldate = {07/05/14},
year = {1991}
}
@misc{Debian2014,
author = {Debian},
title = {{Debian “wheezy” Release Information}},
url = {https://www.debian.org/releases/wheezy/},
urldate = {07/05/14},
year = {2014}
}
@misc{Denning2005,
abstract = {Locality of reference is a fundamental principle of computing with many applications. Here is its story.},
author = {Denning, Peter J.},
booktitle = {Communications of the ACM},
doi = {10.1145/1070838.1070856},
isbn = {0001-0782},
issn = {00010782},
pages = {19},
title = {{The locality principle}},
volume = {48},
year = {2005}
}
@misc{Doweck,
author = {Doweck, Jack},
title = {{Inside Intel® Core™ Microarchitecture}},
url = {http://www.hotchips.org/wp-content/uploads/hc\_archives/hc18/3\_Tues/HC18.S9/HC18.S9T4.pdf},
urldate = {03/06/14},
year = {2006}
}
@article{Doweck2006,
author = {Doweck, Jack},
title = {{Inside Intel® Core™ Microarchitecture and Smart Memory Access}},
year = {2006}
}
@article{DuBois2013,
author = {{Du Bois}, Kristof and Eyerman, Stijn and Eeckhout, Lieven},
doi = {10.1145/2400682.2400688},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Du Bois, Eyerman, Eeckhout/ACM Transactions on Architecture and Code Optimization/Du Bois, Eyerman, Eeckhout - 2013 - Per-thread cycle accounting in multicore processors.pdf:pdf},
issn = {15443566},
journal = {ACM Transactions on Architecture and Code Optimization},
month = jan,
number = {4},
pages = {1--22},
title = {{Per-thread cycle accounting in multicore processors}},
url = {http://dl.acm.org/citation.cfm?doid=2400682.2400688},
volume = {9},
year = {2013}
}
@article{Esmaeilzadeh2013,
abstract = {ments have slowed dramatically. Under these conditions, more cores are only possible if the cores are slower, simpler, or less utilized with each additional technology generation. This paper brings together transistor technology, processor core, and application models to understand whether multicore scaling can sustain the historical exponential performance growth in this energy-limited era. As the number of cores increases, power constraints may prevent powering of all cores at their full speed, requiring a fraction of the cores to be powered off at all times. According to our models, the fraction of these chips that is “dark” may be as much as 50\% within three process generations. The low utility of this “dark silicon” may prevent both scaling to higher core counts and ultimately the economic viability of continued silicon scaling. Our results show that core count scaling provides much less performance gain than conventional wisdom suggests. Under (highly) optimistic scaling assumptions—for parallel workloads—multicore scaling provides a 7.9× (23\% per year) over ten years. Under more conservative (realistic) assump- tions, multicore scaling provides a total performance gain of 3.7× (14\% per year) over ten years, and obviously less when sufficiently parallel workloads are unavailable. Without a breakthrough in process technology or microarchitecture, other directions are needed to continue the historical rate of performance improvement.},
author = {Esmaeilzadeh, Hadi and Blem, Emily and Amant, Ren\'{e}e St. and Sankaralingam, Karthikeyan and Burger, Doug},
doi = {10.1145/2408776.2408797},
issn = {00010782},
journal = {Communications of the ACM},
pages = {93},
title = {{Power challenges may end the multicore era}},
url = {http://dl.acm.org/citation.cfm?doid=2408776.2408797},
volume = {56},
year = {2013}
}
@misc{Faydoc2014,
author = {Faydoc},
title = {{RDTSC - Read Time-Stamp Counter}},
url = {http://faydoc.tripod.com/cpu/rdtsc.htm},
urldate = {08/05/14},
year = {2014}
}
@misc{FreeOnlineDictionary2014,
author = {{Free Online Dictionary}},
keywords = {English dictionary,definition of processor chip,dictionary,encyclopedia,explanation,information,law,legal,medical,online dictionary,processor chip,processor chip definition,term,thesaurus},
title = {{Definition of processor chip by the Free Online Dictionary, Thesaurus and Encyclopedia.}},
url = {http://www.thefreedictionary.com/processor+chip},
urldate = {12/05/14},
year = {2014}
}
@article{Gepner2006,
abstract = {Multi-core processors represent an evolutionary change in conventional computing as well setting the new trend for high performance computing (HPC) - but parallelism is nothing new. Intel has a long history with the concept of parallelism and the development of hardware-enhanced threading capabilities. Intel has been delivering threading-capable products for more than a decade. The move toward chip-level multiprocessing architectures with a large number of cores continues to offer dramatically increased performance and power characteristics. Nonetheless, this move also presents significant challenges. This paper describes how far the industry has progressed and evaluates some of the challenges we are facing with multi-core processors and some of the solutions that have been developed},
author = {Gepner, P. and Kowalik, M.F.},
doi = {10.1109/PARELEC.2006.54},
isbn = {0-7695-2554-7},
journal = {International Symposium on Parallel Computing in Electrical Engineering (PARELEC'06)},
title = {{Multi-Core Processors: New Way to Achieve High System Performance}},
year = {2006}
}
@book{gove2010multicore,
author = {Gove, Darryl},
publisher = {Addison-Wesley Professional},
title = {{Multicore Application Programming: For Windows, Linux, and Oracle Solaris}},
year = {2010}
}
@misc{Granlund2012,
author = {Granlund, Torbjorn},
title = {{Instruction latencies and throughput for AMD and Intel x86 processors}},
url = {https://gmplib.org/~tege/x86-timing.pdf},
urldate = {12/05/14},
year = {2012}
}
@misc{Gruener2012,
author = {Gruener, Wolfgang},
title = {{Intel Has 5 nm Processors in Sight}},
url = {http://www.tomshardware.com/news/intel-cpu-processor-5nm,17578.html},
urldate = {12/05/14},
year = {2012}
}
@article{HasinaKhatoonShahidHafeezMirza2013,
abstract = {The processor-memory speed gap referred to as memory wall, has become much wider in multi core processors due to a number of cores sharing the processor-memory interface. In addition to other cache optimization techniques, the mechanism of prefetching instructions and data has been used effectively to close the processor-memory speed gap and lower the memory wall. A number of issues have emerged when prefetching is used aggressively in multicore processors. The results presented in this paper are an indicator of the problems that need to be taken into consideration while using prefetching as a default technique. This paper also quantifies the amount of degradation that applications face with the aggressive use of prefetching. Another aspect that is investigated is the performance of multicore processors using a multiprogram workload as compared to a single program workload while varying the configuration of the built-in hardware prefetchers. Parallel workloads are also investigated to estimate the speedup and the effect of hardware prefetchers. $\backslash$nThis paper is the outcome of work that forms a part of the PhD research project currently in progress at NED University of Engineering and Technology, Karachi. $\backslash$n},
author = {{Hasina Khatoon Shahid Hafeez Mirza}, Talat Altaf},
journal = {International Journal of Advanced Computer Science and Applications(IJACSA)},
keywords = {Multicore; prefetchers; prefetch-sensitive; memory},
title = {{Exploiting the Role of Hardware Prefetchers in Multicore Processors}},
url = {http://ijacsa.thesai.org/},
volume = {4},
year = {2013}
}
@article{Heidelberger1990,
author = {Heidelberger, Philip and Stone, Harold S.},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Heidelberger, Stone/Unknown/Heidelberger, Stone - 1990 - Parallel trace-driven cache simulation by time partitioning.pdf:pdf},
isbn = {0-911801-72-3},
month = dec,
pages = {734--737},
publisher = {IEEE Press},
title = {{Parallel trace-driven cache simulation by time partitioning}},
url = {http://dl.acm.org/citation.cfm?id=328885.329238},
year = {1990}
}
@book{Hennessy2006,
abstract = {The era of seemingly unlimited growth in processor performance is over: single chip architectures can no longer overcome the performance limitations imposed by the power they consume and the heat they generate. Today, Intel and other semiconductor firms are abandoning the single fast processor model in favor of multi-core microprocessors-chips that combine two or more processors in a single package. In the fourth edition of emphComputer Architecture, the authors focus on this historic shift, increasing their coverage of multiprocessors and exploring the most effective ways of achieving parallelism as the key to unlocking the power of multiple processor architectures. Additionally, the new edition has expanded and updated coverage of design topics beyond processor performance, including power, reliability, availability, and dependability. textbfCD System Requirements emphPDF Viewer The CD material includes PDF documents that you can read with a PDF viewer such as Adobe, Acrobat or Adobe Reader. Recent versions of Adobe Reader for some platforms are included on the CD. emphHTML Browser The navigation framework on this CD is delivered in HTML and JavaScript. It is recommended that you install the latest version of your favorite HTML browser to view this CD. The content has been verified under Windows XP with the following browsers: Internet Explorer 6.0, Firefox 1.5; under Mac OS X (Panther) with the following browsers: Internet Explorer 5.2, Firefox 1.0.6, Safari 1.3; and under Mandriva Linux 2006 with the following browsers: Firefox 1.0.6, Konqueror 3.4.2, Mozilla 1.7.11. The content is designed to be viewed in a browser window that is at least 720 pixels wide. You may find the content does not display well if your display is not set to at least 1024x768 pixel resolution. emphOperating System This CD can be used under any operating system that includes an HTML browser and a PDF viewer. This includes Windows, Mac OS, and most Linux and Unix systems. Increased coverage on achieving parallelism with multiprocessors. Case studies of latest technology from industry including the Sun Niagara Multiprocessor, AMD Opteron, and Pentium 4. Three review appendices, included in the printed volume, review the basic and intermediate principles the main text relies upon. Eight reference appendices, collected on the CD, cover a range of topics including specific architectures, embedded systems, application specific processors-some guest authored by subject experts.},
author = {Hennessy, John L and Patterson, David A},
booktitle = {Microelectronics Journal},
doi = {10.1016/S0026-2692(97)80955-X},
isbn = {0123704901},
issn = {00262692},
pages = {704},
title = {{Computer Architecture A Quantitative Approach 4th Edition}},
url = {http://www.amazon.com/Computer-Architecture-Quantitative-Approach-4th/dp/0123704901},
volume = {28},
year = {2006}
}
@article{Henning2000,
abstract = {As computers and software have become more powerful, it seems
almost human nature to want the biggest and fastest toy you can afford.
But how do you know if your toy is tops? Even if your application never
does any I/O, it's not just the speed of the CPU that dictates
performance. Cache, main memory, and compilers also play a role.
Software applications also have differing performance requirements. So
whom do you trust to provide this information? The Standard Performance
Evaluation Corporation (SPEC) is a nonprofit consortium whose members
include hardware vendors, software vendors, universities, customers, and
consultants. SPEC's mission is to develop technically credible and
objective component- and system-level benchmarks for multiple operating
systems and environments, including high-performance numeric computing,
Web servers, and graphical subsystems. On 30 June 2000, SPEC retired the
CPU95 benchmark suite. Its replacement is CPU2000, a new CPU benchmark
suite with 19 applications that have never before been in a SPEC CPU
suite. The article discusses how SPEC developed this benchmark suite and
what the benchmarks do},
author = {Henning, J.L.},
doi = {10.1109/2.869367},
issn = {0018-9162},
journal = {Computer},
title = {{SPEC CPU2000: measuring CPU performance in the New Millennium}},
volume = {33},
year = {2000}
}
@article{Hill2008,
abstract = {Augmenting Amdahl's law with a corollary for multicore hardware makes it relevant to future generations of chips with multiple processor cores. Obtaining optimal multicore performance will require further research in both extracting more parallelism and making sequential cores faster.},
author = {Hill, Mark D and Marty, Michael R},
doi = {10.1109/MC.2008.209},
isbn = {00189162},
issn = {0018-9162},
journal = {Computer},
pages = {33--38},
title = {{Amdahl ’s Law in the Multicore Era}},
volume = {41},
year = {2008}
}
@misc{IBM2010,
author = {IBM},
title = {{Under the Hood: Of POWER7 Processor Caches}},
url = {http://www-03.ibm.com/systems/resources/systems\_power\_software\_i\_perfmgmt\_underthehood.pdf},
urldate = {12/05/14},
year = {2010}
}
@misc{ICHEC2014b,
author = {ICHEC},
keywords = {HPC,High Performance Computing,ICHEC,Irish Centre for High-End Computing,Scientific Computing,Supercomputer},
title = {{Fionn Supercomputer}},
url = {https://www.ichec.ie/infrastructure/fionn},
urldate = {23/05/14},
year = {2014}
}
@misc{ICHEC2014a,
author = {ICHEC},
keywords = {HPC,High Performance Computing,ICHEC,Irish Centre for High-End Computing,Scientific Computing,Supercomputer},
title = {{Fionn and Stoney Documentation}},
url = {https://www.ichec.ie/support/documentation/},
urldate = {09/05/14},
year = {2014}
}
@misc{ICHEC2014,
author = {ICHEC},
keywords = {HPC,High Performance Computing,ICHEC,Irish Centre for High-End Computing,Scientific Computing,Supercomputer},
title = {{Ireland's High-Performance Computing Centre}},
url = {https://www.ichec.ie/},
urldate = {28/04/14},
year = {2014}
}
@misc{ICPE2014,
author = {ICPE},
keywords = {2014,ICPE,ICPE 2014,International Conference,Karlsruhe,Performance Engineering,SIPEW,SPEC,WOSP},
title = {{International Conference on Performance Engineering: ICPE2014}},
url = {http://icpe2014.ipd.kit.edu/},
urldate = {08/02/14},
year = {2014}
}
@misc{IEEEStd,
author = {{IEEE Std}},
title = {clock\_gettime(3): clock/time functions},
url = {http://linux.die.net/man/3/clock\_gettime},
urldate = {30/04/14}
}
@misc{Anonymous,
author = {{IEEE Std}},
title = {setpriority(3): get/set nice value},
url = {http://linux.die.net/man/3/setpriority},
urldate = {30/03/14},
year = {2003}
}
@misc{Intel,
annote = {RDTSC code},
author = {Intel},
title = {{Using the RDTSC Instruction for Performance Monitoring (press release)}},
url = {http://www.ccsl.carleton.ca/~jamuir/rdtscpm1.pdf},
urldate = {28/03/14}
}
@article{Intel2014,
author = {Intel},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Intel/Unknown/Intel - 2014 - Intel ® 64 and IA-32 Architectures Software Developer’ s Manual.pdf:pdf},
number = {February},
title = {{Intel ® 64 and IA-32 Architectures Software Developer’ s Manual}},
volume = {Volume 3A:},
year = {2014}
}
@misc{Intel2000,
author = {Intel},
keywords = {845,business,celeron,chipset,consumer,desktop,motherboard,pentium,pentium 4,processor},
title = {{Intel Introduces The Pentium® 4 Processor (press release)}},
url = {http://web.archive.org/web/20070403032914/http://www.intel.com/pressroom/archive/releases/dp112000.htm},
urldate = {08/02/14},
year = {2000}
}
@misc{Intel2009a,
author = {Intel},
keywords = {I/O communication,distributed shared memory,high-speed interconnect,integrated memory controller,next-generation interconnect,processor communication,quick path technology,quickpath interconnect},
title = {{An Introduction to the Intel® QuickPath Interconnect (press release)}},
url = {http://www.intel.com/content/www/us/en/io/quickpath-technology/quick-path-interconnect-introduction-paper.html},
urldate = {13/05/14},
year = {2009}
}
@misc{Intel2009,
author = {Intel},
title = {{ARK | Intel® Xeon® Processor L5530 (8M Cache, 2.40 GHz, 5.86 GT/s Intel® QPI)}},
url = {http://ark.intel.com/products/41755/Intel-Xeon-Processor-L5530-8M-Cache-2\_40-GHz-5\_86-GTs-Intel-QPI},
urldate = {28/04/14},
year = {2009}
}
@misc{Intel2014a,
author = {Intel},
keywords = {Intel},
title = {{Homepage of Intel}},
url = {http://www.intel.com},
urldate = {11/06/14},
year = {2014}
}
@misc{Intel2009b,
author = {Intel},
title = {{What you Need to Know about Prefetching (press release)}},
url = {https://software.intel.com/en-us/blogs/2009/08/24/what-you-need-to-know-about-prefetching?language=es},
urldate = {23/05/14},
year = {20}
}
@misc{Intel2013,
author = {Intel},
title = {{Intel® VTune™ Amplifier XE 2013}},
url = {http://software.intel.com/en-us/intel-vtune-amplifier-xe},
urldate = {08/02/14},
year = {2013}
}
@misc{Intel2012,
author = {Intel},
title = {{Software Techniques for Shared-Cache Multi-Core Systems (press release)}},
url = {https://software.intel.com/en-us/articles/software-techniques-for-shared-cache-multi-core-systems/?wapkw=smart+cache},
urldate = {12/05/14},
year = {2012}
}
@misc{Intel2013a,
author = {Intel},
title = {{ARK | Intel® Xeon® Processor E5-2695 v2 (30M Cache, 2.40 GHz)}},
url = {http://ark.intel.com/products/75281/Intel-Xeon-Processor-E5-2695-v2-30M-Cache-2\_40-GHz},
urldate = {28/04/14},
year = {2013}
}
@misc{Intel2006,
author = {Intel},
title = {{Intel® Xeon® Processor 5130}},
url = {http://ark.intel.com/products/27216/},
urldate = {07/02/14},
year = {2006}
}
@article{Iwai2009,
abstract = {The down-scaling is still the most important and effective way for achieving the high-performance logic CMOS operation with low power, regardless of its concern for the technological difficulties, and thus, the past shrinking trend of the gate-length has been very aggressive. In this paper, logic CMOS technology roadmap for ‘22nm and beyond’ is described with ITRS (International Technology Roadmap for Semiconductor) as a reference. In the ITRS 2008 Update published just recently, there has been some significant change in the trend of the gate length. The future gate-length shrinking trend predicted in the past several versions of the ITRS has been too aggressive even for the most advanced semiconductor companies to catch up, and thus, the predicted trend has been amended to be less aggressive from the ITRS 2008 Update, resulting in the delay in the gate-length shrinkage for 3years in the short term and 5years in the long term from those predicted in ITRS 2007. Corresponding to this, the pace of the introduction of new technologies becomes slower. For the long term, the limit of the downsizing is a big concern. The limit is expected to be at the gate length of around 5nm because of the too huge off-leakage current in the entire chip. Until that we will have probably six more generations or ‘technology nodes’, considering that we are now in the so-called 45nm generation. It would take probably 20–30years until we reach the final limit, because the duration between the generations will become longer when approaching the limit. In order to suppress the off-leakage current, double gate (DG) or fin-FET type MOSFETs are the most promising. Then, it is a natural extension for DG FETs to evolve to Si-nanowire MOSFETs as the ultimate structure of transistors for CMOS circuit applications. Si-nanowire FETs are more attractive than the conventional DG FETs because of higher on-current conduction due to their quantum nature and also because of their adoptability for high-density integration including that of 3D. Then, what will come next after reaching the final limit of the downsizing? The answer is new algorithm. In the latter half of this century, the application of algorithm used for the natural bio system such as the brains of insects and even human will make the integrated circuits operation tremendously high efficiency. Much higher performance with ultimately low power consumption will be realized.},
author = {Iwai, H.},
doi = {10.1016/j.mee.2009.03.129},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Iwai/Microelectronic Engineering/Iwai - 2009 - Roadmap for 22nm and beyond (Invited Paper).pdf:pdf},
issn = {01679317},
journal = {Microelectronic Engineering},
keywords = {22nm,CMOS logic,Nanowire,Roadmap,Si},
month = jul,
number = {7-9},
pages = {1520--1528},
title = {{Roadmap for 22nm and beyond (Invited Paper)}},
url = {http://www.sciencedirect.com/science/article/pii/S0167931709002950},
volume = {86},
year = {2009}
}
@article{Jagtap2009,
abstract = {Since the very beginning of integrated circuits development, processors were invented with ever-increasing clock frequencies and sophisticated in-build optimisation strategies. Due to physical limitations, this 'free lunch' of speedup has come to an end. Physical constraints put up multiple barriers in achieving high performance computing within power budgets. Computer architects are trying to cross these performance restricting walls using number of innovative processor architectures employing area concurrency. These processors are commonly known as multi-core processors. The following article gives a summary of recent trends and challenges in multi-core processor architectures. It discusses how 40 years of parallel computing research need to be considered in the upcoming multi-core era. I feel that the future research must be driven from two sides-a better expression of hardware structures, and a domain-specific understanding of software parallelism.},
author = {Jagtap, M.P.},
journal = {DRDO Science Spectrum},
keywords = {high perfomance computing,multi-core processor architecture,multi-core processors},
pages = {87--94},
title = {{Era of Multi-Core Processors}},
url = {http://www.drdo.gov.in/drdo/pub/dss/2009/main/16-ANURAG.pdf},
volume = {2},
year = {2009}
}
@misc{Juurlink2012,
abstract = {Several recent works predict the future of multicore systems or identify scalability bottlenecks based on Amdahl's law. Amdahl's law implicitly assumes, however, that the problem size stays constant, but in most cases more cores are used to solve larger and more complex problems. There is a related law known as Gustafson's law which assumes that runtime, not the problem size, is constant. In other words, it is assumed that the runtime on p cores is the same as the runtime on 1 core and that the parallel part of an application scales linearly with the number of cores. We apply Gustafson's law to symmetric, asymmetric, and dynamic multicores and show that this leads to fundamentally different results than when Amdahl's law is applied. We also generalize Amdahl's and Gustafson's law and study how this quantitatively effects the dimensioning of future multicore systems.},
author = {Juurlink, B.H.H. and Meenderinck, C. H.},
booktitle = {ACM SIGARCH Computer Architecture News},
doi = {10.1145/2234336.2234338},
isbn = {0163-5964},
issn = {01635964},
pages = {1},
title = {{Amdahl's law for predicting the future of multicores considered harmful}},
volume = {40},
year = {2012}
}
@misc{Kankowski2012,
author = {Kankowski, Peter},
title = {{Performance measurements with RDTSC}},
url = {http://www.strchr.com/performance\_measurements\_with\_rdtsc},
urldate = {08/05/14},
year = {2012}
}
@article{Kazempour2008,
author = {Kazempour, Vahid and Fedorova, Alexandra and Alagheband, Pouya},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Kazempour, Fedorova, Alagheband/\ldots Par 2008–Parallel Processing/Kazempour, Fedorova, Alagheband - 2008 - Performance implications of cache affinity on multicore processors.pdf:pdf},
journal = {\ldots Par 2008–Parallel Processing},
keywords = {cache affinity,multicore processors,performance evaluation,scheduling},
title = {{Performance implications of cache affinity on multicore processors}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-85451-7\_17},
year = {2008}
}
@misc{Kazutomo2006,
author = {Kazutomo, Yoshii},
title = {rdtsc},
url = {http://www.mcs.anl.gov/~kazutomo/rdtsc.html},
urldate = {30/04/14},
year = {2006}
}
@misc{KentMilfeldKazushigeGotoAviPurkayastha2007,
author = {{Kent Milfeld, Kazushige Goto, Avi Purkayastha}, Chona Guiang and Karl Schulz},
title = {{Effective Use of Multi-Core Commodity Systems in HPC}},
url = {http://www.linuxclustersinstitute.org/conferences/archive/2007/PDF/milfeld\_24433.pdf},
urldate = {03/06/14},
year = {2007}
}
@misc{Kernel.org2012,
author = {Kernel.org},
title = {{Frequently Asked Questions on Linux Kernel}},
url = {https://rt.wiki.kernel.org/index.php/Frequently\_Asked\_Questions},
urldate = {06/05/14},
year = {2012}
}
@article{Kish2002,
abstract = {The exponential growth of memory size and clock frequency in computers has a great impact on everyday life. The growth is empirically described by Moore's law of miniaturization. Physical limitations of this growth would have a serious impact on technology and economy. A thermodynamical effect, the increasing thermal noise voltage (Johnson–Nyquist noise) on decreasing characteristic capacitances, together with the constrain of using lower supply voltages to keep power dissipation manageable on the contrary of increasing clock frequency, has the potential to break abruptly Moore's law within 6–8 years, or earlier.},
author = {Kish, Laszlo B},
doi = {10.1016/S0375-9601(02)01365-8},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Kish/Physics Letters A/Kish - 2002 - End of Moore's law thermal (noise) death of integration in micro and nano electronics.pdf:pdf},
issn = {03759601},
journal = {Physics Letters A},
month = dec,
number = {3-4},
pages = {144--149},
title = {{End of Moore's law: thermal (noise) death of integration in micro and nano electronics}},
url = {http://www.sciencedirect.com/science/article/pii/S0375960102013658},
volume = {305},
year = {2002}
}
@article{Kwak1999,
annote = {Outdated},
author = {Kwak, Hantak and Lee, Ben and Hurson, Ali R},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Kwak, Lee, Hurson/Unknown/Kwak, Lee, Hurson - 1999 - Effects of Multithreading on Cache Performance.pdf:pdf},
number = {2},
pages = {176--184},
title = {{Effects of Multithreading on Cache Performance}},
volume = {48},
year = {1999}
}
@misc{Langston2007,
author = {Langston, Jeremy W. and He, Xubin},
title = {{Multi-core Processors and Caching - A Survey}},
url = {http://blogs.cae.tntech.edu/jwlangston21/files/2008/08/multi-core-processors-and-caching-a-survey-ieee-format.pdf},
urldate = {13/05/14},
year = {2007}
}
@article{Laukkanen2011,
abstract = {Context: Defect reporting is an important part of software development in-vivo, but previous work from open source context suggests that defect reports often have insufficient information for defect fixing. Objective: Our goal was to reproduce and partially replicate one of those open source studies in industrial context to see how well the results could be generalized. Method: We surveyed developers from six industrial software development organizations about the defect report information, from three viewpoints: concerning quality, usefulness and automation possibilities of the information. Seventy-four developers out of 142 completed our survey. Results: Our reproduction confirms the results of the prior study in that "steps to reproduce" and "observed behaviour" are highly important defect information. Our results extend the results of the prior study as we found that "part of the application", "configuration of the application", and "operating data" are also highly important, but they were not surveyed in the prior study. Finally, we classified defect information as "critical problems", "solutions", "boosters", and "essentials" based on the survey answers. Conclusion: The quality of defect reports is a problem in the software industry as well as in the open source community. Thus, we suggest that a part of the defect reporting should be automated since many of the defect reporters lack technical knowledge or interest to produce high-quality defect reports.},
author = {Laukkanen, Eero I. and Mantyla, Mika V.},
doi = {10.1109/ESEM.2011.28},
isbn = {978-0-7695-4604-9},
issn = {1938-6451},
journal = {2011 International Symposium on Empirical Software Engineering and Measurement},
keywords = {software debugging,software maintenance,software quality},
pages = {197--206},
title = {{Survey Reproduction of Defect Reporting in Industrial Software Development}},
year = {2011}
}
@inproceedings{Lee2006,
abstract = {Sub-5nm all-around gate FinFETs with 3nm fin width were fabricated for the first time. The n-channel FinFET of sub-5nm with 1.4nm HfO2  shows an IDsat of 497muA/mum at VG=V D=1.0V. Characteristics of sub-5nm transistor are verified by using 3-D simulations as well as analytical models. A threshold voltage increases as the fin width reduces by quantum confinement effects. The threshold voltage shift was fitted to a theoretical model with consideration of the first-order perturbation theory. And a channel orientation effect, based on a current-flow direction, is shown},
author = {Lee, H. and Yu, L.-E. and Ryu, S.-W. and Han, J.-W. and Jeon, K. and Jang, D.-Y. and Kim, K.-H. and Lee, J. and Kim, J.-H. and Jeon, S. and Lee, G. and Oh, J. and Park, Y. and Bae, W. and Yang, J. and Yoo, J. and Kim, S. and Choi, Y.-K.},
booktitle = {2006 Symposium on VLSI Technology, 2006. Digest of Technical Papers.},
doi = {10.1109/VLSIT.2006.1705215},
isbn = {1-4244-0005-8},
pages = {58--59},
publisher = {IEEE},
shorttitle = {VLSI Technology, 2006. Digest of Technical Papers.},
title = {{Sub-5nm All-Around Gate FinFET for Ultimate Scaling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1705215},
year = {2006}
}
@misc{Levinthal,
author = {Levinthal, David},
title = {{Performance Analysis Guide for Intel® Core™ i7 Processor and Intel® Xeon™ 5500 processors}},
url = {https://software.intel.com/sites/products/collateral/hpc/vtune/performance\_analysis\_guide.pdf},
urldate = {28/04/14},
year = {2009}
}
@book{Lewis1995,
author = {Lewis, Bill and Press, SunSoft and Berg, Daniel J.},
isbn = {0134436989},
pages = {352},
publisher = {Prentice Hall PTR},
title = {{Threads Primer: A Guide to Multithreaded Programming}},
url = {http://www.amazon.com/Threads-Primer-Guide-Multithreaded-Programming/dp/0134436989},
year = {1995}
}
@inproceedings{Liaskovitis2006,
abstract = {In chip multiprocessors (CMPs), limiting the number of off-chip cache misses is crucial for good performance. Many multithreaded programs provide opportunities for  constructive cache sharing , in which concurrently scheduled threads share a largely overlapping working set. In this brief announcement, we highlight our ongoing study [4] comparing the performance of two schedulers designed for fine-grained multithreaded programs: Parallel Depth First (PDF) [2], which is designed for constructive sharing, and Work Stealing (WS) [3], which takes a more traditional approach. Overview of schedulers.  In PDF, processing cores are allocated ready-to-execute program tasks such that higher scheduling priority is given to those tasks the sequential program would have executed earlier. As a result, PDF tends to co-schedule threads in a way that tracks the sequential execution. Hence, the aggregate working set is (provably) not much larger than the single thread working set [1]. In WS, each processing core maintains a local work queue of readyto-execute threads. Whenever its local queue is empty, the core steals a thread from the bottom of the first non-empty queue it finds. WS is an attractive scheduling policy because when there is plenty of parallelism, stealing is quite rare. However, WS is not designed for constructive cache sharing, because the cores tend to have disjoint working sets. CMP configurations studied.  We evaluated the performance of PDF and WS across a range of simulated CMP configurations. We focused on designs that have fixed-size private L1 caches and a shared L2 cache on chip. For a fixed die size (240 mm2), we varied the number of cores from 1 to 32. For a given number of cores, we used a (default) configuration based on current CMPs and realistic projections of future CMPs, as process technologies decrease from 90nm to 32nm. Summary of findings.  We studied a variety of benchmark programs to show the following findings.For several application classes, PDF enables significant constructive sharing between threads, leading to better utilization of the on-chip caches and reducing off-chip traffic compared to WS. In particular, bandwidth-limited irregular programs and parallel divide-and-conquer programs present a relative speedup of 1.3-1.6X over WS, observing a 13- 41\% reduction in off-chip traffic. An example is shown in Figure 1, for parallel merge sort. For each schedule, the number of L2 misses (i.e., the off-chip traffic) is shown on the left and the speed-up over running on one core is shown on the right, for 1 to 32 cores. Note that reducing the offchip traffic has the additional benefit of reducing the power consumption. Moreover, PDF's smaller working sets provide opportunities to power down segments of the cache without increasing the running time. Furthermore, when multiple programs are active concurrently, the PDF version is also less of a cache hog and its smaller working set is more likely to remain in the cache across context switches.For several other applications classes, PDF and WS have roughly the same execution times, either because there is only limited data reuse that can be exploited or because the programs are not limited by off-chip bandwidth. In the latter case, the constructive sharing PDF enables does provide the power and multiprogramming benefits discussed above.Finally, most parallel benchmarks to date, written for SMPs, use such a coarse-grained threading that they cannot exploit the constructive cache behavior inherent in PDF.We find that mechanisms to finely grain multithreaded applications are crucial to achieving good performance on CMPs.},
author = {Liaskovitis, Vasileios and Mowry, Todd C. and Wilkerson, Chris and Chen, Shimin and Gibbons, Phillip B. and Ailamaki, Anastassia and Blelloch, Guy E. and Falsafi, Babak and Fix, Limor and Hardavellas, Nikos and Kozuch, Michael},
booktitle = {Proceedings of the eighteenth annual ACM symposium on Parallelism in algorithms and architectures - SPAA '06},
doi = {10.1145/1148109.1148167},
isbn = {1595934529},
keywords = {caches,chip multiprocessors,scheduling},
pages = {330},
title = {{Parallel depth first vs. work stealing schedulers on CMP architectures}},
url = {http://dl.acm.org/citation.cfm?id=1148109.1148167},
year = {2006}
}
@inproceedings{Lin2006,
author = {Lin, Yuan},
booktitle = {Hot Chips: A Symposium on High Performance Chips},
title = {{Multithreaded Programming. Challenges, current practice, and languages/tools support}},
url = {http://www.hotchips.org/wp-content/uploads/hc\_archives/hc18/1\_Sun/HC18.T1P1.pdf},
year = {2006}
}
@misc{LinuxMannual,
author = {{Linux Mannual}},
title = {{pthread\_yield(3) - Linux manual page}},
url = {http://man7.org/linux/man-pages/man3/pthread\_yield.3.html},
urldate = {01/06/14}
}
@misc{LinuxManual2012,
author = {{Linux Manual}},
title = {{pthread\_create(3) - Linux manual page}},
url = {http://man7.org/linux/man-pages/man3/pthread\_create.3.html},
urldate = {08/05/14},
year = {2012}
}
@misc{Manual2012,
author = {{Linux Manual}},
title = {{fopen(3) - Linux manual page}},
url = {http://man7.org/linux/man-pages/man3/fopen.3.html},
urldate = {14/05/14},
year = {2012}
}
@misc{Lmbench1998,
author = {Lmbench},
title = {{LAT\_MEM\_RD(8) - LMBENCH man page}},
url = {http://www.bitmover.com/lmbench/lat\_mem\_rd.8.html},
urldate = {09/05/14},
year = {1998}
}
@article{Lo2013,
abstract = {Although in a multi-threaded processor, the processor may execute more than one process simultaneously to maximize the overall throughput of the system, the executing processes may compete with each other in using shared caches of the processor. This can seriously affect the average performance of the processes as the probability of cache hit for each process could be lowered. In this paper, we propose a new algorithm called the sharable cache partitioning algorithm (ShaParti), for scheduling the processor caches amongst co-running processes. In ShaParti, each executing process has its own cache and a priority scheme is designed for them to share the caches belonging to other executing processes. The performance goals of ShaParti are to improve the cache hit rates of the processes and at the same time the cache miss rates of other concurrent processes will not be lowered compared with the case in which each process has its own cache. Extensive experiments have been performed to illustrate the effectiveness of ShaParti in improving the performance in accessing shared processor caches.},
author = {Lo, Shi-Wu and Lam, Kam-Yiu and Huang, Wen-Yan and Qiu, Sheng-Feng},
doi = {10.1016/j.sysarc.2012.11.005},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Lo et al/Journal of Systems Architecture/Lo et al. - 2013 - An effective cache scheduling scheme for improving the performance in multi-threaded processors.pdf:pdf},
issn = {13837621},
journal = {Journal of Systems Architecture},
keywords = {Cache management,Cache scheduling,Multi-threaded processors,Processor cache},
month = apr,
number = {4-5},
pages = {271--278},
title = {{An effective cache scheduling scheme for improving the performance in multi-threaded processors}},
url = {http://www.sciencedirect.com/science/article/pii/S1383762112001051},
volume = {59},
year = {2013}
}
@article{lundstrom2003moore,
author = {Lundstrom, Mark},
journal = {SCIENCE-NEW YORK THEN WASHINGTON-},
pages = {210--212},
publisher = {American Association for the Advancement of Science},
title = {{Moore's law forever?}},
year = {2003}
}
@article{Luo2004,
abstract = { Simulation is the most important tool for computer architects to evaluate the performance of new computer designs. However, detailed simulation is extremely time consuming. Sampling is one of the techniques that effectively reduce simulation time. In order to achieve accurate sampling results, microarchitectural structure must be adequately warmed up before each measurement. In this paper, a new technique for warming up microprocessor caches is proposed. The simulator monitors the warm-up process of the caches and decides when the caches are warmed up based on simple heuristics. In our experiments the self-monitored adaptive (SMA) warm-up technique on average exhibits only 0.2\% warm-up error in CPI. SMA achieves smaller warm-up error with only 1/2-1/3 of the warm-up length of previous methods. In addition, it is adaptive to the cache configuration simulated. For simulating small caches, the SMA technique can reduce the warm-up overhead by an order of magnitude compared to previous techniques. Finally, SMA gives the user some indicator of warm-up error at the end of the cycle-accurate simulation that helps the user to gauge the accuracy of the warm-up.},
author = {Luo, Y. and John, L.K. and Eeckhout, L.},
doi = {10.1109/SBAC-PAD.2004.38},
isbn = {0-7695-2240-8},
issn = {1550-6533},
journal = {16th Symposium on Computer Architecture and High Performance Computing},
title = {{Self-monitored adaptive cache warm-up for microprocessor simulation}},
year = {2004}
}
@inproceedings{McKee2004,
abstract = {This paper looks at the evolution of the “Memory Wall” problem over the past decade. It begins by reviewing the short Computer Architecture News note that coined the phrase, including the motivation behind the note, the context in which it was written, and the controversy it sparked. What has changed over the years? Are we hitting the Mem- ory Wall? And if so, for what types of applications?},
author = {McKee, Sally a.},
booktitle = {Proceedings of the first conference on computing frontiers on Computing frontiers - CF'04},
doi = {10.1145/977091.977115},
isbn = {1581137419},
keywords = {memory performance,system balance},
pages = {162},
title = {{Reflections on the memory wall}},
url = {http://portal.acm.org/citation.cfm?doid=977091.977115},
year = {2004}
}
@misc{McVoy2012,
author = {McVoy, Larry},
title = {{LMbench - Tools for Performance Analysis}},
url = {http://www.bitmover.com/lmbench/},
urldate = {08/02/14},
year = {2012}
}
@article{Mogul1997,
author = {Mogul, Jeffrey C. and Ramakrishnan, K. K.},
doi = {10.1145/263326.263335},
issn = {07342071},
journal = {ACM Transactions on Computer Systems},
keywords = {interrupt-driven kernel,livelock,polling,scheduling},
month = aug,
number = {3},
pages = {217--252},
publisher = {ACM},
title = {{Eliminating receive livelock in an interrupt-driven kernel}},
url = {http://dl.acm.org/citation.cfm?id=263326.263335},
volume = {15},
year = {1997}
}
@inproceedings{Molka2009a,
abstract = {Today's microprocessors have complex memory subsystems with several cache levels. The efficient use of this memory hierarchy is crucial to gain optimal performance, especially on multicore processors. Unfortunately, many implementation details of these processors are not publicly available. In this paper we present such fundamental details of the newly introduced Intel Nehalem microarchitecture with its integrated memory controller, quick path interconnect, and ccNUMA architecture. Our analysis is based on sophisticated benchmarks to measure the latency and bandwidth between different locations in the memory subsystem. Special care is taken to control the coherency state of the data to gain insight into performance relevant implementation details of the cache coherency protocol. Based on these benchmarks we present undocumented performance data and architectural properties.},
author = {Molka, Daniel and Hackenberg, Daniel and Schone, Robert and Muller, Matthias S.},
booktitle = {Parallel Architectures and Compilation Techniques - Conference Proceedings, PACT},
doi = {10.1109/PACT.2009.22},
isbn = {9780769537719},
issn = {1089795X},
keywords = {Bandwidth,Cache coherency,Latency,Multicore,Nehalem},
pages = {261--270},
title = {{Memory performance and cache coherency effects on an intel nehalem multiprocessor system}},
year = {2009}
}
@article{Molka2009,
author = {Molka, Daniel and Hackenberg, Daniel and Schone, Robert and Muller, Matthias S.},
doi = {10.1109/PACT.2009.22},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Molka et al/2009 18th International Conference on Parallel Architectures and Compilation Techniques/Molka et al. - 2009 - Memory Performance and Cache Coherency Effects on an Intel Nehalem Multiprocessor System.pdf:pdf},
isbn = {978-0-7695-3771-9},
journal = {2009 18th International Conference on Parallel Architectures and Compilation Techniques},
keywords = {-nehalem,bandwidth,cache coherency,multicore},
month = sep,
pages = {261--270},
publisher = {Ieee},
title = {{Memory Performance and Cache Coherency Effects on an Intel Nehalem Multiprocessor System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5260544},
year = {2009}
}
@article{Moore1998,
abstract = {With unit cost falling as the number of components per$\backslash$ncircuit rises, by 1975 economics may dictate squeezing as$\backslash$nmany as 65,000 components on a single silicon chip},
author = {Moore, G.E.},
doi = {10.1109/JPROC.1998.658762},
isbn = {1558605398},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
pmid = {21527652},
title = {{Cramming More Components Onto Integrated Circuits}},
volume = {86},
year = {1998}
}
@article{Nayfeh1997,
abstract = {Presents the case for billion-transistor processor architectures
that will consist of chip multiprocessors (CMPs): multiple (four to 16)
simple, fast processors on one chip. In their proposal, each processor
is tightly coupled to a small, fast, level-one cache, and all processors
share a larger level-two cache. The processors may collaborate on a
parallel job or run independent tasks (as in the SMT proposal). The CMP
architecture lends itself to simpler design, faster validation, cleaner
functional partitioning, and higher theoretical peak performance.
However for this architecture to realize its performance potential,
either programmers or compilers will have to make code explicitly
parallel. Old ISAs will be incompatible with this architecture (although
they could run slowly on one of the small processors)},
author = {Nayfeh, B.A. and Olukotun, K.},
doi = {10.1109/2.612253},
issn = {0018-9162},
journal = {Computer},
title = {{A single-chip multiprocessor}},
volume = {30},
year = {1997}
}
@misc{Neupane2004,
author = {Neupane, Mahesh},
title = {{Cache Coherence}},
url = {http://cse.csusb.edu/schubert/tutorials/csci610/w04/MN\_Cache\_Coherence.pdf},
urldate = {13/05/14},
year = {2004}
}
@misc{Openet2014,
author = {Openet},
title = {{Homepage of Openet}},
url = {http://www.openet.com/},
urldate = {11/06/14},
year = {2014}
}
@misc{Ostrovsky2010,
author = {Ostrovsky, Igor},
title = {{Gallery of Processor Cache Effects}},
url = {http://igoro.com/archive/gallery-of-processor-cache-effects/},
urldate = {12/05/14},
year = {2010}
}
@inproceedings{Ottoni2005,
abstract = {Until recently, a steadily rising clock rate and other uniprocessor micro architectural improvements could be relied upon to consistently deliver increasing performance for a wide range of applications. Current difficulties in maintaining this trend have lead microprocessor manufacturers to add value by incorporating multiple processors on a chip. Unfortunately, since decades of compiler research have not succeeded in delivering automatic threading for prevalent code properties, this approach demonstrates no improvement for a large class of existing codes. To find useful work for chip multiprocessors, we propose an automatic approach to thread extraction, called decoupled software pipelining (DSWP). DSWP exploits the finegrained pipeline parallelism lurking in most applications to extract long-running, concurrently executing threads. Use of the nonspeculative and truly decoupled threads produced by DSWP can increase execution efficiency and provide significant latency tolerance, mitigating design complexity by reducing intercore communication and per-core resource requirements. Using our initial fully automatic compiler implementation and a validated processor model, we prove the concept by demonstrating significant gains for dual-core chip multiprocessor models running a variety of codes. We then explore simple opportunities missed by our initial compiler implementation which suggest a promising future for this approach.},
author = {Ottoni, G. and Rangan, R. and Stoler, A. and August, D.I.},
booktitle = {38th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO'05)},
doi = {10.1109/MICRO.2005.13},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Ottoni et al/38th Annual IEEEACM International Symposium on Microarchitecture (MICRO'05)/Ottoni et al. - 2005 - Automatic Thread Extraction with Decoupled Software Pipelining.pdf:pdf},
isbn = {0-7695-2440-0},
keywords = {Application software,Clocks,Delay,Hardware,Manufacturing processes,Microarchitecture,Microprocessors,Parallel processing,Pipeline processing,Yarn,automatic compiler implementation,automatic thread extraction,chip multiprocessors,decoupled software pipelining,dual-core chip multiprocessor models,extraction threading,intercore communication,microprocessor chips,multi-threading,multiprocessing systems,per-core resource requirements,processor model,program compilers},
pages = {105--118},
publisher = {IEEE},
shorttitle = {Microarchitecture, 2005. MICRO-38. Proceedings. 38},
title = {{Automatic Thread Extraction with Decoupled Software Pipelining}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1540952},
year = {2005}
}
@inproceedings{ousterhout1996threads,
author = {Ousterhout, John},
booktitle = {Presentation given at the 1996 Usenix Annual Technical Conference},
organization = {San Diego, CA, USA},
title = {{Why threads are a bad idea (for most purposes)}},
volume = {5},
year = {1996}
}
@article{Panda1999,
abstract = {Loop blocking (tiling) is a well-known compiler optimization that helps improve cache performance by dividing the loop iteration space into smaller blocks (tiles); reuse of array elements within each tile is maximized by ensuring that the working set for the tile fits into the data cache. Padding is a data alignment technique that involves the insertion of dummy elements into a data structure for improving cache performance. In this work, we present DAT, a technique that augments loop tiling with data alignment, achieving improved efficiency (by ensuring that the cache is never under-utilized) as well as improved flexibility (by eliminating self-interference cache conflicts independent of the tile size). This results in a more stable and better cache performance than existing approaches, in addition to maximizing cache utilization, eliminating self-interference, and minimizing cross-interference conflicts. Further, while all previous efforts are targeted at programs characterized by the reuse of a single array, we also address the issue of minimizing conflict misses when several tiled arrays are involved. To validate our technique, we ran extensive experiments using both simulations as well as actual measurements on SUN Sparc5 and Sparc10 workstations. The results on benchmarks exhibiting varying memory access patterns demonstrate the effectiveness of our technique through consistently high hit ratios and improved performance across varying problem sizes},
author = {Panda, P.R. and Nakamura, H. and Dutt, N.D. and Nicolau, A.},
doi = {10.1109/12.752655},
issn = {00189340},
journal = {IEEE Transactions on Computers},
keywords = {Cache memory,Data structures,Helium,Logic arrays,Optimizing compilers,Prefetching,Radio access networks,Sparc10,Sparc5,Sun,Tiles,Workstations,cache performance,cache storage,cache utilization,compiler optimization,data alignment,loop iteration,loop tiling,optimising compilers,performance evaluation},
number = {2},
pages = {142--149},
shorttitle = {Computers, IEEE Transactions on},
title = {{Augmenting loop tiling with data alignment for improved cache performance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=752655},
volume = {48},
year = {1999}
}
@misc{Paolini2010,
author = {Paolini, Gabriele},
keywords = {324264,3954,CPUID,Gabriele Paoloni,RTDSCP,benchmark},
title = {{Code Execution Times: IA-32/IA-64 Instruction Set Architecture}},
url = {http://www.intel.com/content/www/us/en/intelligent-systems/embedded-systems-training/ia-32-ia-64-benchmark-code-execution-paper.html},
urldate = {29/04/14},
year = {2010}
}
@article{Peter2010,
author = {Peter, Simon and Sch\"{u}pbach, Adrian and Barham, Paul and Baumann, Andrew and Isaacs, Rebecca and Harris, Tim and Roscoe, Timothy},
month = jun,
pages = {10},
publisher = {USENIX Association},
title = {{Design principles for end-to-end multicore schedulers}},
url = {http://dl.acm.org/citation.cfm?id=1863086.1863096},
year = {2010}
}
@inproceedings{Philippaerts2013,
abstract = {In this article, we present a series of four industrial case studies in software verification. We applied VeriFast, a sound and modular software verifier based on separation logic, to two Java Card smart card applets, a Linux device driver, and an embedded Linux network management component, the latter two written in C. Our case studies have been carefully selected so as to evaluate the industrial applicability of VeriFast. We focus on proving the absence of safety violations, e.g., that the programs do not perform illegal operations such as dividing by zero or illegal memory accesses. Yet, given the sensitive application environment of our case studies, these safety properties typically have security implications. In this article we give a detailed description of the VeriFast approach to software verification based on two of the above case studies, one in Java and one in C. Finally, we draw conclusions on the overall feasibility of using VeriFast to verify software components in industrial domains that have stringent requirements on reliability and security. © 2013 Elsevier B.V. All rights reserved.},
author = {Philippaerts, Pieter and M\"{u}hlberg, Jan Tobias and Penninckx, Willem and Smans, Jan and Jacobs, Bart and Piessens, Frank},
booktitle = {Science of Computer Programming},
doi = {10.1016/j.scico.2013.01.006},
issn = {01676423},
keywords = {Industrial case studies,Software verification,VeriFast},
title = {{Software verification with VeriFast: Industrial case studies}},
year = {2013}
}
@book{citeulike:200722,
abstract = {The mark of a craftsman is his familiarity with his tools, the speed with which he can use them to solve simple problems, and his cleverness in using them to solve more complicated challenges. The latest edition of <I>Unix Power Tools</I> explores the standard Unix tools in greater depth than ever, and with better coverage of Linux, FreeBSD, and even the Darwin environment of Mac OS X. It's also been improved by the addition of sections on Perl and Python, programming languages that can often solve Unix problems more adeptly than any specific utility. This detail-filled book distinguishes itself from other guides for Unix gurus with its organizational structure (it's a series of articles that can be absorbed sequentially or individually) and carefully designed and executed index. Like its esteemed predecessors, this book is one you will keep handy.<p>The authors have achieved a nearly ideal balance in the pages of this book. It's not just a collection of recipes (such collections tend to leave you hanging if you want to do something a little differently), it's not just a book of documentation (books like that have application mainly as references for people who know a lot already), and it's not just a conceptual how-to guide. <I>Unix Power Tools</I> is all of those things, and the overall effect is impressive indeed. If you work with any flavor of Unix, whatever your level of experience, you will benefit by having this book. <I>--David Wall</I> <p> <B>Topics covered:</B> How to work efficiently, elegantly, and creatively with the Unix tool suite, as well as (to a lesser extent) with Perl and Python scripts. Tips and strategies on customization, document generation, process management, and networking abound in this wisdom-rich volume.},
author = {Powers, Shelley and Peek, Jerry and O'Reilly, Tim and Loukides, Mike},
edition = {3rd},
howpublished = {Paperback},
isbn = {0596003307},
keywords = {unix},
month = oct,
publisher = {O'Reilly Media, Inc.},
title = {{Unix Power Tools, Third Edition}},
url = {http://www.worldcat.org/isbn/0596003307},
year = {2002}
}
@phdthesis{Prakash2007,
author = {Prakash, Tribuvan Kumar},
school = {Louisiana State University},
title = {{Performance Analysis of Intel Core 2 Duo Processor}},
url = {http://www.bioperf.org/Pra07.pdf},
year = {2007}
}
@article{Pusukuri,
author = {Pusukuri, Kishore Kumar},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Pusukuri/Unknown/Pusukuri - 2013 - A ADAPT A Framework for Coscheduling Multithreaded Programs.pdf:pdf},
title = {{A ADAPT: A Framework for Coscheduling Multithreaded Programs}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.366.5818},
year = {2013}
}
@inproceedings{Putigny2014,
author = {Putigny, Bertrand and Goglin, Brice and Barthou, Denis},
booktitle = {International Conference on High Performance Computing \& Simulation},
title = {{A Benchmark-based Performance Model for Memory-bound HPC Applications}},
url = {http://hal.archives-ouvertes.fr/docs/00/98/55/98/PDF/benchmark\_based\_memory\_model.pdf},
year = {2014}
}
@inproceedings{RanjanPanda1997,
abstract = {We address the problem of improving the data cache performance of numerical applications-specifically, those with blocked (or tiled) loops. We present DAT, a data alignment technique utilizing array-padding, to improve program performance through minimizing cache conflict misses. We describe algorithms for selecting tile sizes for maximizing data cache utilization, and computing pad sizes for eliminating self-interference conflicts in the chosen tile. We also present a generalization of the technique to handle applications with several tiled arrays. Our experimental results comparing our technique with previous published approaches on machines with different cache configurations show consistently good performance on several benchmark programs, for a variety of problem sizes},
author = {{Ranjan Panda}, P. and Nakamura, H. and Dutt, N.D. and Nicolau, A.},
booktitle = {Proceedings International Conference on Computer Design VLSI in Computers and Processors},
doi = {10.1109/ICCD.1997.628925},
isbn = {0-8186-8206-X},
issn = {1063-6404},
keywords = {Application software,Cache memory,Computer science,Degradation,Electronic switching systems,Interference,Logic arrays,Optimizing compilers,Prefetching,Tiles,array-padding,benchmark programs,cache performance,cache storage,data alignment technique,fault tolerant computing,pad sizes,performance evaluation,program performance},
pages = {587--592},
publisher = {IEEE Comput. Soc},
shorttitle = {Computer Design: VLSI in Computers and Processors,},
title = {{A data alignment technique for improving cache performance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=628925},
year = {1997}
}
@book{rao2008computer,
author = {RAO, P V S},
publisher = {PHI Learning Pvt. Ltd.},
title = {{Computer system architecture}},
year = {2008}
}
@inproceedings{Roehm2013,
abstract = {The first step to comprehend and fix a software bug is usually to reproduce the corresponding failure. Reproducing a failure requires information about steps to reproduce, i.e. the steps necessary to make a failure occur in the development environment. In case of an application with a user interface, steps to reproduce consist of the interactions between a user and the application that precede the failure. Unfortunately, bug reports typically lack this information. Users are either unaware of its importance to developers, are unable to describe it, or simply do not have time to report it. In this paper, we present a simple but effective and resource efficient approach to monitor interactions between users and their applications selectively at a high level of abstraction, e.g. editing operations and commands. This minimizes the monitoring overhead and enables developers to analyze user interaction traces. We map monitored interactions to a taxonomy of user interactions to help developers comprehend user behavior. Further, we present the Timeline Tool that visualizes monitored interaction traces preceding failures. To evaluate our approach we conducted an experiment with 12 participants and asked them to reproduce bug reports from an open-source project. We found that developers are able to derive steps to reproduce from monitored interaction traces. In particular, inexperienced developers profit from the Timeline Tool, as they are able to reproduce failures that they cannot reproduce without it. The monitoring overhead is rather small (approx. 5\% CPU and 2-5\% memory) and users feel it does not influence their work in a negative way.},
author = {Roehm, Tobias and Gurbanova, Nigar and Bruegge, Bernd and Joubert, Christophe and Maalej, Walid},
booktitle = {2013 21st International Conference on Program Comprehension (ICPC)},
doi = {10.1109/ICPC.2013.6613835},
isbn = {978-1-4673-3092-3},
issn = {1063-6897},
keywords = {Application instrumentation,Bug fixing,Failure reproduction,Monitoring,Program comprehension,Sensors,Servers,Software,Software evolution,Software maintenance,Steps to reproduce,Taxonomy,Trace analysis,Unified modeling language,User monitoring,Visualization,abstraction level,data visualisation,development environment,editing commands,editing operations,monitored interaction traces,open-source project,program debugging,public domain software,resource efficient approach,software bug,software fault tolerance,timeline tool,user interactions monitoring,user interface,user interfaces},
pages = {73--82},
title = {{Monitoring user interactions for supporting failure reproduction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6613835},
year = {2013}
}
@misc{RogersTimothyG.OConnorMike,
author = {{Rogers Timothy G., O’Connor Mike}, Aamodt Tor M.},
title = {{Cache-Conscious Wavefront Scheduling}},
url = {http://delivery.acm.org/10.1145/2460000/2457487/4924a072.pdf?ip=149.157.117.221\&id=2457487\&acc=ACTIVE SERVICE\&key=C2716FEBFA981EF1C517B38E12D13249503B474E5DC23DD5\&CFID=407204912\&CFTOKEN=44916331\&\_\_acm\_\_=1391675251\_ac479d9e628268ab098c4d66fc21b222},
urldate = {06/02/14},
year = {2012}
}
@article{Rogers2013,
abstract = {Highly multithreaded architectures introduce another dimension to fine-grained hardware cache management. The order in which the system's threads issue instructions can significantly impact the access stream seen by the caching system. This article studies a set of economically important server applications and presents the cache-conscious wavefront scheduling (CCWS) hardware mechanism, which uses feedback from the memory system to guide the issue-level thread scheduler and shape the access pattern seen by the first-level cache.},
author = {Rogers, Timothy G. and O'Connor, Mike and Aamodt, Tor M.},
doi = {10.1109/MM.2013.24},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Rogers, O'Connor, Aamodt/IEEE Micro/Rogers, O'Connor, Aamodt - 2013 - Cache-Conscious Thread Scheduling for Massively Multithreaded Processors.pdf:pdf},
issn = {0272-1732},
journal = {IEEE Micro},
keywords = {CCWS,Cache storage,Computer architecture,GPU,Hardware,Memory management,Multithreading,Parallel processing,Program processors,SIMD processors,Scheduling,access stream,cache,cache-conscious thread scheduling,cache-conscious wavefront scheduling,cache-conscious wavefront scheduling hardware mech,caching system,fine-grained hardware cache management,highly multithreaded architectures,issue-level thread scheduler,locality,massively multithreaded processors,memory system feedback,memory systems,microprocessor chips,multi-threading,parallel architectures,parallel processors,processor scheduling,thread scheduling},
month = may,
number = {3},
pages = {78--85},
shorttitle = {Micro, IEEE},
title = {{Cache-Conscious Thread Scheduling for Massively Multithreaded Processors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6487475},
volume = {33},
year = {2013}
}
@misc{Ruggiero2008,
author = {Ruggiero, Joshua},
title = {{Measuring Cache and Memory Latency and CPU to Memory Bandwidth}},
url = {http://www.intel.la/content/dam/www/public/us/en/documents/white-papers/ia-cache-latency-bandwidth-paper.pdf},
urldate = {26/03/14},
year = {2008}
}
@article{schauer2008multicore,
author = {Schauer, Bryan},
journal = {ProQuest Discovery Guides1--14},
title = {{Multicore processors--A necessity}},
year = {2008}
}
@misc{Schmid2004,
author = {Schmid, Patrick},
title = {{NetBurst Architecture: Now 31 Pipeline Stages - Intel's New Weapon: Pentium 4 Prescott}},
url = {http://www.tomshardware.com/reviews/intel,751-5.html},
urldate = {08/02/14},
year = {2004}
}
@inproceedings{Schneider2006,
address = {New York, New York, USA},
author = {Schneider, Scott and Antonopoulos, Christos D. and Nikolopoulos, Dimitrios S.},
booktitle = {Proceedings of the 2006 international symposium on Memory management - ISMM '06},
doi = {10.1145/1133956.1133968},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Schneider, Antonopoulos, Nikolopoulos/Proceedings of the 2006 international symposium on Memory management - ISMM '06/Schneider, Antonopoulos, Nikolopoulos - 2006 - Scalable locality-conscious multithreaded memory allocation.pdf:pdf},
isbn = {1595932216},
keywords = {memory management,multithreading,non-blocking,shared memory,synchronization-free},
month = jun,
pages = {84},
publisher = {ACM Press},
title = {{Scalable locality-conscious multithreaded memory allocation}},
url = {http://dl.acm.org/citation.cfm?id=1133956.1133968},
year = {2006}
}
@phdthesis{Shen2000,
author = {Shen, Xiaowei},
school = {Massachusetts Institute of Technology},
title = {{Design and Verification of Adaptive Cache Coherence Protocols}},
url = {http://csg.csail.mit.edu/pubs/memos/Memo-471/memo471.pdf},
year = {2000}
}
@article{Song2007,
abstract = {It is critical to provide high performance for scientific applications running on chip multi-processors (CMP). A CMP architecture often comprises a shared 12 cache and lower-level storages. The shared 12 cache can reduce the number of cache misses if the data are accessed in common by several threads, but it can also lead to performance degradation due to resource contention. Sometimes running threads on all cores can cause severe contention and increase the number of cache misses greatly. To investigate how the performance of a thread varies when running it concurrently with other threads on the remaining cores, we develop an analytical model to predict the number of misses on the shared L2 cache. In particular, we apply the model to thread-parallel numerical pro grams. We assume that all the threads compute homogeneous tasks and share a fully associative L2 cache. We use circular sequence profiling and stack processing techniques to analyze the L2 cache trace to predict the number of compulsory cache misses, capacity cache misses on shared data, and capacity cache misses on private data, respectively. Our method is able to predict the L2 cache performance for threads that have a global shared address space. For scientific applications, threads often have overlapping memory footprints. We use a cycle accurate simulator to validate the model with three scientific programs: dense matrix multiplication, blocked dense matrix multiplication, and sparse matrix-vector product. The average relative errors for the three experiments are 8.01\%, 1.85\%, and 2.41\%, respectively.},
author = {Song, Fengguang Song Fengguang and Moore, S. and Dongarra, J.},
doi = {10.1109/ICPP.2007.52},
isbn = {978-0-7695-2933-2},
issn = {0190-3918},
journal = {2007 International Conference on Parallel Processing (ICPP 2007)},
keywords = {architecture,cache,chip multi-processor,multi-threaded programming,performance modeling},
title = {{L2 Cache Modeling for Scientific Applications on Chip Multi-Processors}},
year = {2007}
}
@misc{SPEC2014,
author = {SPEC},
title = {{SPEC - Standard Performance Evaluation Corporation}},
url = {http://www.spec.org/},
urldate = {08/02/14},
year = {2014}
}
@misc{SPEC,
author = {SPEC},
keywords = {CPU2000,SPEC,SPECfp2000,SPECfp\_base2000,SPECint2000,SPECint\_base2000,SPECint\_rate2000,benchmarks,performance},
title = {{SPEC CPU2000}},
url = {https://www.spec.org/cpu2000/},
urldate = {20/02/14},
year = {2007}
}
@article{Squillante1993,
abstract = {In a shared-memory multiprocessor system, it may be more efficient to schedule a task on one processor than on another if relevant data already reside in a particular processor's cache. The effects of this type of processor affinity are examined. It is observed that tasks continuously alternate between executing at a processor and releasing this processor due to I/O, synchronization, quantum expiration, or preemption. Queuing network models of different abstract scheduling policies are formulated, spanning the range from ignoring affinity to fixing tasks on processors. These models are solved via mean value analysis, where possible, and by simulation otherwise. An analytic cache model is developed and used in these scheduling models to include the effects of an initial burst of cache misses experienced by tasks when they return to a processor for execution. A mean-value technique is also developed and used in the scheduling models to include the effects of increased bus traffic due to these bursts of cache misses. Only a small amount of affinity information needs to be maintained for each task. The importance of having a policy that adapts its behavior to changes in system load is demonstrated},
author = {Squillante, M.S. and Lazowska, E.D.},
doi = {10.1109/71.207589},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Analytical models,Degradation,I/O,Information analysis,Measurement,Multiprocessing systems,Operating systems,Performance analysis,Processor scheduling,Queueing analysis,Traffic control,analytic cache model,buffer storage,mean value analysis,performance evaluation,preemption,processor-cache affinity information,quantum expiration,queueing network models,queueing theory,scheduling,shared memory systems,shared-memory multiprocessor scheduling,synchronization},
number = {2},
pages = {131--143},
shorttitle = {Parallel and Distributed Systems, IEEE Transaction},
title = {{Using processor-cache affinity information in shared-memory multiprocessor scheduling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=207589},
volume = {4},
year = {1993}
}
@inproceedings{Su1995,
address = {New York, New York, USA},
author = {Su, Ching-Long and Despain, Alvin M.},
booktitle = {Proceedings of the 1995 international symposium on Low power design - ISLPED '95},
doi = {10.1145/224081.224093},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Su, Despain/Proceedings of the 1995 international symposium on Low power design - ISLPED '95/Su, Despain - 1995 - Cache design trade-offs for power and performance optimization.pdf:pdf},
isbn = {0897917448},
month = apr,
pages = {63--68},
publisher = {ACM Press},
title = {{Cache design trade-offs for power and performance optimization}},
url = {http://dl.acm.org/citation.cfm?id=224081.224093},
year = {1995}
}
@misc{SUSE2014,
author = {SUSE},
keywords = {linux server},
title = {{SUSE Linux Enterprise Server}},
url = {https://www.suse.com/products/server/},
urldate = {07/05/14},
year = {2014}
}
@inproceedings{thakkar1990performance,
author = {Thakkar, Shreekant S and Sweiger, Mark},
booktitle = {Computer Architecture, 1990. Proceedings., 17th Annual International Symposium on},
organization = {IEEE},
pages = {228--238},
title = {{Performance of an OLTP application on Symmetry multiprocessor system}},
year = {1990}
}
@misc{TheLinuxInformationProject2006,
author = {{The Linux Information Project}},
title = {{Context Switch definition}},
url = {http://www.linfo.org/context\_switch.html},
urldate = {08/05/14},
year = {2006}
}
@misc{Tian2012,
author = {Tian, Tian and Shih, Chiu-Pi},
title = {{Software Techniques for Shared-Cache Multi-Core Systems}},
url = {http://software.intel.com/en-us/articles/software-techniques-for-shared-cache-multi-core-systems},
year = {2012}
}
@article{Tiwari1998,
abstract = {Power consumption has become one of the biggest challenges in high-performance microprocessor design. The rapid increase in the complexity and speed of each new CPU generation is outstripping the benefits of voltage reduction and feature size scaling. Designers are thus continuously challenged to come up with innovative ways to reduce power, while trying to meet all the other constraints imposed on the design. This paper presents an overview of the issues related to power consumption in the context of Intel CPUs. The main trends that are driving the increased focus on design for low power are described. System and benchmarking issues, and sources of power consumption in a high-performance CPU are briefly described. Techniques that have been tried on real designs in the past are described. The role of CAD tools and their limitations in this domain are also discussed. In addition, areas that need increased research focus in the future are also pointed out.},
archivePrefix = {arXiv},
arxivId = {6084487},
author = {Tiwari, V. and Singh, D. and Rajgopal, S. and Mehta, G. and Patel, R. and Baez, F.},
doi = {10.1109/DAC.1998.136625},
eprint = {6084487},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Tiwari et al/Proceedings 1998 Design and Automation Conference. 35th DAC. (Cat. No.98CH36175)/Tiwari et al. - 1998 - Reducing power in high-performance microprocessors.pdf:pdf},
isbn = {0-89791-964-5},
journal = {Proceedings 1998 Design and Automation Conference. 35th DAC. (Cat. No.98CH36175)},
title = {{Reducing power in high-performance microprocessors}},
year = {1998}
}
@article{Torrellas1995,
abstract = {As a process executes on a processor, it builds up state in that processor′s cache. In multiprogrammed workloads, the opportunity to reuse this state may be lost when a process gets rescheduled, either because intervening processes destroy its cache state or because the process may migrate to another processor. In this paper, we explore affinity scheduling, a technique that helps reduce cache misses by preferentially scheduling a process on a processor where it has run recently. Our study focuses on a bus-based multiprocessor executing a variety of workloads, including mixes of scientific, software development, and database applications. In addition to quantifying the performance benefits of exploiting affinity, our study is distinctive in that it provides low-level data from a hardware performance monitor that details why the workloads perform as they do. Overall, for the workloads studied, we show that affinity scheduling reduces the number of cache misses by 7-36\%, resulting in execution time improvements of up to 10\%. Although the overall improvements are small, modifying the operating system scheduler to exploit affinity appears worthwhile-affinity has no negative impact on the workloads and we show that it is extremely simple to add to existing schedulers.},
author = {Torrellas, J. and Tucker, A. and Gupta, A.},
doi = {10.1006/jpdc.1995.1014},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Torrellas, Tucker, Gupta/Journal of Parallel and Distributed Computing/Torrellas, Tucker, Gupta - 1995 - Evaluating the Performance of Cache-Affinity Scheduling in Shared-Memory Multiprocessors.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
month = feb,
number = {2},
pages = {139--151},
title = {{Evaluating the Performance of Cache-Affinity Scheduling in Shared-Memory Multiprocessors}},
url = {http://www.sciencedirect.com/science/article/pii/S0743731585710143},
volume = {24},
year = {1995}
}
@article{torrellas1995evaluating,
author = {Torrellas, Josep and Tucker, Andrew and Gupta, Anoop},
journal = {Journal of Parallel and Distributed Computing},
number = {2},
pages = {139--151},
publisher = {Elsevier},
title = {{Evaluating the performance of cache-affinity scheduling in shared-memory multiprocessors}},
volume = {24},
year = {1995}
}
@misc{Valgrind2010,
author = {Valgrind},
keywords = {Valgrind Memcheck Cachegrind Callgrind Massif Helg},
title = {{Valgrind's homepage}},
url = {http://valgrind.org/},
urldate = {08/02/14},
year = {2010}
}
@article{Vaswani1991,
author = {Vaswani, Raj and Zahorjan, John},
doi = {10.1145/121133.121140},
file = {:Users/pavlo/Dropbox/Publications/Mendeley/Vaswani, Zahorjan/ACM SIGOPS Operating Systems Review/Vaswani, Zahorjan - 1991 - The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors.pdf:pdf},
isbn = {0-89791-447-3},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
month = oct,
number = {5},
pages = {26--40},
publisher = {ACM},
title = {{The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors}},
url = {http://dl.acm.org/citation.cfm?id=121133.121140},
volume = {25},
year = {1991}
}
@article{Vera2004,
abstract = {Data caches are a key hardware means to bridge the gap between processor and memory speeds, but only for programs that exhibit sufficient data locality in their memory accesses. Thus, a method for evaluating cache performance is required to both determine quantitatively cache misses and to guide data cache optimizations. Existing analytical models for data cache optimizations target mainly isolated perfect loop nests. We present an analytical model that is capable of statically analyzing not only loop nest fragments, but also complete numerical programs with regular and compile-time predictable memory accesses. Central to the whole-program approach are abstract call inlining, memory access vectors, and parametric reuse analysis, which allow the reuse and interference both within and across loop nests to be quantified precisely in a unified framework. Based on the framework, the cache misses of a program are specified using mathematical formulas and the miss ratio is predicted from these formulas based on statistical sampling techniques. Our experimental results using kernels and whole programs indicate accurate cache miss estimates in a substantially shorter amount of time (typically, several orders of magnitude faster) than simulation.},
author = {Vera, X.},
doi = {10.1109/TC.2004.1275296},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
keywords = {Modeling techniques,analytical modeling,cache memories,data locality,performance evaluation.},
language = {English},
month = may,
number = {5},
pages = {547--566\_3},
publisher = {IEEE Computer Society},
title = {{Efficient and accurate analytical modeling of whole-program data cache behavior}},
url = {http://www.computer.org/csdl/trans/tc/2004/05/t0547.html},
volume = {53},
year = {2004}
}
@inproceedings{VineethMekkatAnupHoleyPen-ChungYew2013,
author = {{Vineeth Mekkat, Anup Holey, Pen-Chung Yew}, Antonia Zhai},
booktitle = {PACT 2013},
title = {{Managing Shared Last-Level Cache in a Heterogeneous Multicore Processor}},
url = {http://www-users.cs.umn.edu/~mekkat/pact2013-mekkat.pdf},
year = {2013}
}
@misc{Wall1991,
abstract = {Growing interest in ambitious multiple-issue machines and heavily -pipelined machines requires a careful examination of how much instruction-level parallelism exists in typical programs. Such an examination is complicated by the wide variety of hardware and software techniques for increasing the parallelism that can be exploited, including branch prediction, register renaming, and alias analysis. By performing simulations based on instruction maces, we can model techniques at the limits of feasibility and even beyond. Our study shows a striking difference between assuming that the techniques we use are perfect and merely assuming that they are impossibly good. Even with impossibly good techniques, average parallelism rarely exceeds 7, with 5 more},
author = {Wall, David W.},
booktitle = {ACM SIGARCH Computer Architecture News},
doi = {10.1145/106975.106991},
isbn = {0-89791-380-9},
issn = {01635964},
pages = {176--188},
title = {{Limits of instruction-level parallelism}},
volume = {19},
year = {1991}
}
@book{ward1990computation,
author = {Ward, Stephen A and Halstead, Robert H},
publisher = {MIT press},
title = {{Computation structures}},
year = {1990}
}
@inproceedings{Weaver2013,
author = {Weaver, Vincent M. and Terpstra, Dan and Moore, Shirley},
booktitle = {2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
doi = {10.1109/ISPASS.2013.6557172},
isbn = {978-1-4673-5779-1},
keywords = {ARM,Assembly,Benchmark testing,Hardware,IA64 system,Kernel,PMU implementations,POWER system,Phasor measurement units,Radiation detectors,SPARC system,architectural redesign,computer architecture,counter event,deterministic replay library implementation,deterministic threading library implementation,hardware performance counter implementation,ideal hardware performance counters,multiprocessing systems,nondeterminism,overcount,performance evaluation,performance monitoring unit,run-to-run variation,x86\_64 CPU implementations},
language = {English},
month = apr,
pages = {215--224},
publisher = {IEEE},
title = {{Non-determinism and overcount on modern hardware performance counter implementations}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6557172},
year = {2013}
}
@misc{Weisner2012,
author = {Weisner, Rickey C.},
title = {{How Memory Allocation Affects Performance in Multi-Threaded Programs}},
url = {http://www.oracle.com/technetwork/articles/servers-storage-dev/mem-alloc-1557798.html},
year = {2012}
}
@article{Woodcock2009,
abstract = {Formal methods use mathematical models for analysis and verification at any part of the program life-cycle. We describe the state of the art in the industrial use of formal methods, concentrating on their increasing use at the earlier stages of specification and design. We do this by reporting on a new survey of industrial use, comparing the situation in 2009 with the most significant surveys carried out over the last 20 years. We describe some of the highlights of our survey by presenting a series of industrial projects, and we draw some observations from these surveys and records of experience. Based on this, we discuss the issues surrounding the industrial adoption of formal methods. Finally, we look to the future and describe the development of a Verified Software Repository, part of the worldwide Verified Software Initiative. We introduce the initial projects being used to populate the repository, and describe the challenges they address.},
author = {Woodcock, Jim and Larsen, Peter Gorm and Bicarregui, Juan and Fitzgerald, John},
doi = {10.1145/1592434.1592436},
issn = {03600300},
journal = {ACM Computing Surveys},
pages = {1----36},
title = {{Formal methods: Practice and experience}},
url = {http://epubs.cclrc.ac.uk/work-details?w=50220},
volume = {41},
year = {2009}
}
@misc{Zhang2010,
abstract = {Most modern Chip Multiprocessors (CMP) feature shared cache on chip. For multithreaded applications, the sharing reduces communication latency among co-running threads, but also results in cache contention. A number of studies have examined the influence of cache sharing on multithreaded applications, but most of them have concentrated on the design or management of shared cache, rather than a systematic measurement of the influence. Consequently, prior measurements have been constrained by the reliance on simulators, the use of out-of-date benchmarks, and the limited coverage of deciding factors. The influence of CMP cache sharing on contemporary multithreaded applications remains preliminarily understood. In this work, we conduct a systematic measurement of the influence on two kinds of commodity CMP machines, using a recently released CMP benchmark suite, PARSEC, with a number of potentially important factors on program, OS, and architecture levels considered. The measurement shows some surprising results. Contrary to commonly perceived importance of cache sharing, neither positive nor negative effects from the cache sharing are significant for most of the program executions, regardless of the types of parallelism, input datasets, architectures, numbers of threads, and assignments of threads to cores. After a detailed analysis, we find that the main reason is the mismatch of current development and compilation of multithreaded applications and CMP architectures. By transforming the programs in a cache-sharing-aware manner, we observe up to 36\% performance increase when the threads are placed on cores appropriately.},
author = {Zhang, Eddy Z. and Jiang, Yunlian and Shen, Xipeng},
booktitle = {ACM SIGPLAN Notices},
doi = {10.1145/1837853.1693482},
isbn = {9781605587080},
issn = {03621340},
pages = {203},
title = {{Does cache sharing on modern CMP matter to the performance of contemporary multithreaded programs?}},
volume = {45},
year = {2010}
}
@misc{Zhao2011,
abstract = {In today’s multi-core systems, cache contention due to true and false sharing can cause unexpected and significant performance degradation. A detailed understanding of a given multi-threaded application’s behavior is required to precisely identify such perfor- mance bottlenecks. Traditionally, however, such diagnostic infor-mation can only be obtained after lengthy simulation of the memory hierarchy. In this paper, we present a novel approach that efficiently analyzes interactions between threads to determine thread correlation and detect true and false sharing. It is based on the following key insight: although the slowdown caused by cache contention depends on factors including the thread-to-core binding and parameters of the memory hierarchy, the amount of data sharing is primarily a function of the cache line size and application behavior. Using memory shadowing and dynamic instrumentation, we implemented a tool that obtains detailed sharing information between threads without simulating the full complexity of the memory hierarchy. The runtime overhead of our approach—a 5× slowdown on average relative to native execution—is significantly less than that of detailed cache simulation. The information collected allows programmers to identify the degree of cache contention in an application, the correlation among its threads, and the sources of significant false sharing. Using our approach, we were able to improve the performance of some applications by up to a factor of 12×. For other contention-intensive applications, we were able to shed light on the obstacles that prevent their performance from scaling to many cores.},
author = {Zhao, Qin and Koh, David and Raza, Syed and Bruening, Derek and Wong, Weng-Fai and Amarasinghe, Saman},
booktitle = {ACM SIGPLAN Notices},
doi = {10.1145/2007477.1952688},
isbn = {9781450305013},
issn = {03621340},
pages = {27},
title = {{Dynamic cache contention detection in multi-threaded applications}},
volume = {46},
year = {2011}
}
@article{Zhuravlev2012,
author = {Zhuravlev, Sergey and Saez, Juan Carlos and Blagodurov, Sergey and Fedorova, Alexandra and Prieto, Manuel},
doi = {10.1145/2379776.2379780},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {Survey,cooperative resource sharing,power-aware scheduling,shared resource contention,thermal effects,thread level scheduling},
month = nov,
number = {1},
pages = {1--28},
publisher = {ACM},
title = {{Survey of scheduling techniques for addressing shared resources in multicore processors}},
url = {http://dl.acm.org/citation.cfm?id=2379776.2379780},
volume = {45},
year = {2012}
}
@article{Zimmermann2010,
abstract = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.},
author = {Zimmermann, T. and Premraj, R. and Bettenburg, N. and Just, S. and Schroter, A. and Weiss, C.},
doi = {10.1109/TSE.2010.63},
isbn = {9781595939951},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Testing and debugging,and enhancement,distribution,human factors,maintenance,management,measurement.},
title = {{What Makes a Good Bug Report?}},
volume = {36},
year = {2010}
}
